# Cross-Lingual SAE Analysis for Vision-Language Model Gender Bias

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## Overview

This project uses **Sparse Autoencoders (SAEs)** to perform mechanistic interpretability analysis on **PaLiGemma-3B** for understanding cross-lingual gender bias in Arabic-English image captioning.

### Key Findings

| Metric | Value | Interpretation |
|--------|-------|----------------|
| **Feature Overlap** | 0.4% | Arabic and English use almost entirely separate gender features |
| **CLBAS Score** | 0.025 | Very low cross-lingual bias alignment |
| **Arabic Probe Accuracy** | 88.5% | Gender is linearly encoded in SAE features |
| **English Probe Accuracy** | 85.3% | Slightly lower than Arabic |

**Novel Finding**: The model develops **language-specific gender circuits** rather than a shared universal gender representation.

## Research Questions

| # | Question | Status |
|---|----------|--------|
| RQ1 | Where do gender representations diverge between Arabic and English? | âœ… All layers show near-complete divergence |
| RQ2 | Are there language-specific gender features? | âœ… 99.6% of features are language-specific |
| RQ3 | Can we steer the model to reduce bias? | ðŸ”„ SBI experiments in progress |
| RQ4 | Grammatical vs semantic gender differences? | âœ… Arabic shows stronger encoding (88.5% vs 85.3%) |

## Project Structure

\`\`\`
sae_captioning_project/
â”œâ”€â”€ README.md                 # This file
â”œâ”€â”€ RESEARCH_PLAN.md          # Detailed research methodology
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ setup.py                  # Package installation
â”‚
â”œâ”€â”€ configs/                  # Configuration files
â”‚   â”œâ”€â”€ config.yaml          # Main configuration
â”‚   â””â”€â”€ clmb_config.yaml     # CLMB framework settings
â”‚
â”œâ”€â”€ scripts/                  # Pipeline scripts (numbered)
â”‚   â”œâ”€â”€ 01_prepare_data.py   # Dataset preparation
â”‚   â”œâ”€â”€ 02_extract_activations.py
â”‚   â”œâ”€â”€ 03_train_sae.py      # SAE training
â”‚   â”œâ”€â”€ 24_cross_lingual_overlap.py    # Feature overlap analysis
â”‚   â”œâ”€â”€ 25_cross_lingual_feature_interpretation.py
â”‚   â”œâ”€â”€ 26_surgical_bias_intervention.py  # SBI experiments
â”‚   â””â”€â”€ slurm_*.sh           # SLURM job scripts
â”‚
â”œâ”€â”€ src/                      # Source code
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ sae.py           # SAE architecture (2048 â†’ 16384)
â”‚   â”‚   â””â”€â”€ hooks.py         # Activation hooks
â”‚   â”œâ”€â”€ clmb/                # Novel CLMB framework
â”‚   â”‚   â”œâ”€â”€ hbl.py           # Hierarchical Bias Localization
â”‚   â”‚   â”œâ”€â”€ clfa.py          # Cross-Lingual Feature Alignment
â”‚   â”‚   â””â”€â”€ sbi.py           # Surgical Bias Intervention
â”‚   â””â”€â”€ analysis/            # Analysis utilities
â”‚
â”œâ”€â”€ docs/                     # Documentation
â”‚   â”œâ”€â”€ guides/              # User guides
â”‚   â”œâ”€â”€ status/              # Pipeline status reports
â”‚   â””â”€â”€ CLMB_FRAMEWORK.md    # Framework documentation
â”‚
â”œâ”€â”€ results/                  # Analysis outputs
â”‚   â”œâ”€â”€ cross_lingual_overlap/
â”‚   â”œâ”€â”€ feature_interpretation/
â”‚   â””â”€â”€ sbi_analysis/
â”‚
â””â”€â”€ visualizations/           # Generated plots
\`\`\`

## Installation

\`\`\`bash
# Clone the repository
git clone https://github.com/nour-mubarak/mechanistic_intrep.git
cd mechanistic_intrep/sae_captioning_project

# Create virtual environment
python -m venv venv
source venv/bin/activate

# Install dependencies
pip install -r requirements.txt
pip install -e .
\`\`\`

## Quick Start

### On NCC Cluster (Durham)

\`\`\`bash
# Full pipeline with SLURM
sbatch scripts/slurm_00_full_pipeline.sh

# Or run individual analysis
sbatch scripts/slurm_24_cross_lingual_overlap.sh
\`\`\`

## Key Results

### Cross-Lingual Feature Overlap

| Layer | Overlap % | CLBAS Score |
|-------|-----------|-------------|
| 0 | 0.0% | 0.013 |
| 3 | 0.0% | 0.011 |
| 6 | 0.0% | 0.015 |
| 9 | 2.0% | 0.028 |
| 12 | 1.0% | 0.039 |
| 15 | 0.0% | 0.028 |
| 17 | 0.0% | 0.041 |

## CLMB Framework

Our novel **Cross-Lingual Mechanistic Bias (CLMB)** framework:

1. **HBL**: Hierarchical Bias Localization
2. **CLFA**: Cross-Lingual Feature Alignment
3. **SBI**: Surgical Bias Intervention
4. **CLBAS**: Cross-Lingual Bias Alignment Score

## License

MIT License
